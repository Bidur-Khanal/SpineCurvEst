{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset: ##\n",
    "\n",
    "Generate bounding box groundtruth for object detection. An image consists of 17 vertebrae, each considered as different objects. Using the corner landmark-coordinates, bound each object with rectangular box. Every image will have 17 objects, each belonging to  different class (17 classes in total), with 4 bounding box coordinates for each object. This script will output a csv file with column headers as: <br/>``` [ image_name, xmin, ymin, xmax, ymax, label ] ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualize the bounding boxes\n",
    "\n",
    "filename = \"sunhl-1th-10-Jan-2017-254 A AP.jpg\"\n",
    "image_directory= \"C:/Users/Brinda Khanal/Downloads/scoliosis xray Single View/boostnet_labeldata/data/training/\"\n",
    "label_directory= \"C:/Users/Brinda Khanal/Downloads/scoliosis xray Single View/boostnet_labeldata/labels/training/\"\n",
    "image = image_directory+filename\n",
    "\n",
    "\n",
    "img = cv2.imread(image)\n",
    "\n",
    "data= pd.read_csv(label_directory+\"landmarks.csv\",header= None)\n",
    "filename_labels= pd.read_csv(label_directory+\"filenames.csv\",header= None)\n",
    "\n",
    "print (\"image shape:\",img.shape)\n",
    "indx= filename_labels[filename_labels.iloc[:,0]== filename].index.tolist()\n",
    "landmark= data.iloc[indx[0]].values\n",
    "for m in range(0,68):\n",
    "    cv2.circle(img,(int(img.shape[1]*landmark[m]),int(img.shape[0]*landmark[m+68])), 10, (255,255,255), -1)\n",
    "\n",
    "landmark = [[int(round(img.shape[1]*landmark[m])),int(round(img.shape[0]*landmark[m+68]))] for m in range (0,68)]\n",
    "#print (landmark)\n",
    "\n",
    "\n",
    "N=4    \n",
    "corners = [landmark[n:n+N] for n in range(0, len(landmark), N)]\n",
    "corners= np.array(corners)\n",
    "\n",
    "avg=[]\n",
    "for box in corners:\n",
    "    x,y,w,h = cv2.boundingRect(box)    \n",
    "    cv2.rectangle(img,(x-10,y-10),(x+w+10,y+h+10),(0,255,0),5)\n",
    "    avg.append((w+10,h+10))\n",
    "    print (x-10,y-10,w+10,h+10)\n",
    "    \n",
    "print (np.mean(avg,axis=0))\n",
    "\n",
    "plt.figure(1, figsize=(25,25))\n",
    "\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.imshow(img[:,:,::-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all the bounding boxes to visiualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_all_bounding_box(image_directory, filenames_csv, landmarks_csv, save_path, split_type= 'train'):\n",
    "    \n",
    "    landmarks_data= pd.read_csv(landmarks_csv,header= None)\n",
    "    filename_labels= pd.read_csv(filenames_csv,header= None)\n",
    "\n",
    "    \n",
    "    for i, names in enumerate(filename_labels.iloc[:,0]):\n",
    "\n",
    "        img = cv2.imread(image_directory+names)\n",
    "        print (image_directory+names)\n",
    "        print (names)\n",
    "        #print (\"image shape\",img.shape)\n",
    "        landmarks = landmarks_data.loc[i].values\n",
    "        landmarks = [[int(round(img.shape[1]*landmarks[m])),int(round(img.shape[0]*landmarks[m+68]))] for m in range (0,68)]\n",
    "        \n",
    "        # group landmark coordinates, each group has 4 points that represents a vertebra\n",
    "        N=4     \n",
    "        box = [landmarks[n:n+N] for n in range(0, len(landmarks), N)]\n",
    "        #print (box)\n",
    "        box= np.array(box)\n",
    "        \n",
    "        for c, box_coordinates in enumerate(box):\n",
    "            x,y,w,h = cv2.boundingRect(box_coordinates) \n",
    "            cv2.rectangle(img,(x-10,y-10),(x+w+10,y+h+10),(0,255,0),5)\n",
    "        cv2.imwrite(save_path+split_type+'/'+names,img)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"C:/Users/Brinda Khanal/Documents/Bidur Git Repo/Spine_Challenge/all landmark estimation/\"\n",
    "train_data_directory = os.path.join(ROOT_PATH, \"groundtruth for 68 landmarks detection/train/\")\n",
    "#val_data_directory = os.path.join(ROOT_PATH, \"data/val/\")\n",
    "train_label_directory=os.path.join(ROOT_PATH, \"groundtruth for 68 landmarks detection/\")\n",
    "#val_label_directory =os.path.join(ROOT_PATH, \"data/labels/val/\")\n",
    "save_path=\"C:/Users/Brinda Khanal/Documents/Bidur Git Repo/Spine_Challenge/all landmark estimation/visualize boxes/\"\n",
    "\n",
    "### call make_csv function to create dataset in format supported by luminoth library\n",
    "\n",
    "visualize_all_bounding_box(train_data_directory,os.path.join(train_label_directory,'train_filenames.csv'),\n",
    "                           os.path.join(train_label_directory,'predicted_train_landmarks.csv'),save_path, 'train')\n",
    "\n",
    "\n",
    "#visualize_all_bounding_box(val_data_directory,os.path.join(val_label_directory,'filenames.csv'),\n",
    "#                           os.path.join(val_label_directory,'landmarks.csv'),save_path, 'val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv_bounding_box(image_directory, filenames_csv, landmarks_csv, split_type= 'train'):\n",
    "    \n",
    "    landmarks_data= pd.read_csv(landmarks_csv,header= None)\n",
    "    filename_labels= pd.read_csv(filenames_csv,header= None)\n",
    "    \n",
    "    df= pd.DataFrame(columns=['image_id', 'xmin','ymin','xmax','ymax','label'])\n",
    "\n",
    "    \n",
    "    for i, names in enumerate(filename_labels.iloc[:,0]):\n",
    "\n",
    "        img = cv2.imread(image_directory+names)\n",
    "        print (names)\n",
    "        #print (\"image shape\",img.shape)\n",
    "        landmarks = landmarks_data.loc[i].values\n",
    "        landmarks = [[int(round(img.shape[1]*landmarks[m])),int(round(img.shape[0]*landmarks[m+68]))] for m in range (0,68)]\n",
    "        \n",
    "        # group landmark coordinates, each group has 4 points that represents a vertebra\n",
    "        N=4     \n",
    "        box = [landmarks[n:n+N] for n in range(0, len(landmarks), N)]\n",
    "        #print (box)\n",
    "        box= np.array(box)\n",
    "        \n",
    "        for c, box_coordinates in enumerate(box):\n",
    "            x,y,w,h = cv2.boundingRect(box_coordinates)\n",
    "            if c < 12:\n",
    "                df= df.append({'image_id': names, 'xmin': x-50, 'ymin': y-10, \n",
    "                               'xmax': x+w+50,'ymax':y+h+10, 'label':1}, ignore_index=True) # increase the area of bounding rectangle if required\n",
    "            else:\n",
    "                df= df.append({'image_id': names, 'xmin': x-50, 'ymin': y-10, \n",
    "                               'xmax': x+w+50,'ymax':y+h+10, 'label':2}, ignore_index=True) # increase the area of bounding rectangle if required\n",
    "                \n",
    "    csv_file= split_type + \".csv\"\n",
    "    df.to_csv(csv_file,index= False)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"C:/Users/Brinda Khanal/Documents/Bidur Git Repo/Spine_Challenge/Object detection/\"\n",
    "train_data_directory = os.path.join(ROOT_PATH, \"data/train/\")\n",
    "val_data_directory = os.path.join(ROOT_PATH, \"data/val/\")\n",
    "train_label_directory=os.path.join(ROOT_PATH, \"data/labels/train/\")\n",
    "val_label_directory =os.path.join(ROOT_PATH, \"data/labels/val/\")\n",
    "\n",
    "### call make_csv function to create dataset in format supported by luminoth library\n",
    "\n",
    "make_csv_bounding_box(train_data_directory,os.path.join(train_label_directory,'filenames.csv'),\n",
    "         os.path.join(train_label_directory,'landmarks.csv'), 'train')\n",
    "\n",
    "\n",
    "make_csv_bounding_box(val_data_directory,os.path.join(val_label_directory,'filenames.csv'),\n",
    "         os.path.join(val_label_directory,'landmarks.csv'), 'val')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate GroundTruth for Landmark Prediction from Patch ##\n",
    " Using each vertebra bounding-box, generate patch-image (1 image will generate 17 patch-images). Find landmark-groundtruth-coordinates (between 0 and 1) for each patch.Save the groundtruth and patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv_landmark(image_directory, filenames_csv, landmarks_csv, split_type= 'train'):\n",
    "    \n",
    "    landmarks_data= pd.read_csv(landmarks_csv,header= None)\n",
    "    filename_labels= pd.read_csv(filenames_csv,header= None)\n",
    "    \n",
    "    df= pd.DataFrame(columns=['image_id', 'x1','y1','x2','y2','x3','y3','x4','y4'])\n",
    "\n",
    "    \n",
    "    for i, names in enumerate(filename_labels.iloc[:,0]):\n",
    "\n",
    "        img = cv2.imread(image_directory+names)\n",
    "        \n",
    "        landmarks = landmarks_data.loc[i].values\n",
    "        landmarks = [[int(round(img.shape[1]*landmarks[m])),int(round(img.shape[0]*landmarks[m+68]))] for m in range (0,68)]\n",
    "        \n",
    "        # group landmark coordinates, each group has 4 points that represents a vertebra\n",
    "        N=4     \n",
    "        box = [landmarks[n:n+N] for n in range(0, len(landmarks), N)]\n",
    "        #print (box)\n",
    "        box= np.array(box)\n",
    "        \n",
    "        \n",
    "        for c, box_coordinates in enumerate(box):\n",
    "            print (box_coordinates)\n",
    "            x_,y_,w_,h_ = cv2.boundingRect(box_coordinates)\n",
    "            \n",
    "            _increase_w = 50 #increase bounding box by certain pixels \n",
    "            _increase_h = 10\n",
    "            \n",
    "            # if increasing bounding box result in region outside the image,donot perform increment\n",
    "            if (x_-_increase_w) <0 :\n",
    "                _increase_w=0\n",
    "            if (y_-_increase_h) <0 :\n",
    "                _increase_h=0\n",
    "            \n",
    "            \n",
    "            patch_image = crop_patch (img, x_,y_,w_,h_, increase_w=_increase_w, increase_h=_increase_h)\n",
    "            \n",
    "            adjusted_landmarks = adjust_landmarks_position(patch_image, box_coordinates, increase_w= _increase_w,\n",
    "                                        increase_h= _increase_h ,x=x_,y=y_,w=w_, h= h_)\n",
    "            \n",
    "            \n",
    "            patch_name = names.replace('.jpg', '')+'_'+str(c)+'.jpg' # filename for each patch\n",
    "            print (\"For Patch: \",patch_name)\n",
    "            \n",
    "           \n",
    "            #resized_patch= cv2.resize(patch_image,(200,120),interpolation=cv2.INTER_AREA) #resize the patches to a fixed size\n",
    "            \n",
    "            \n",
    "            # create a dictionary to append into dataframe row\n",
    "            adjusted_landmarks_= adjusted_landmarks.ravel()\n",
    "            adjusted_landmarks_=np.append(adjusted_landmarks_,patch_name)\n",
    "            keywords= ['x1','y1','x2','y2','x3','y3','x4','y4','image_id']\n",
    "            adjusted_landmarks_dict= dict(zip(keywords,adjusted_landmarks_))\n",
    "            \n",
    "            \n",
    "            print (adjusted_landmarks_dict)\n",
    "            \n",
    "            ''''# for visualization of landmark\n",
    "            adjusted_landmarks[:,0]= adjusted_landmarks[:,0]*resized_patch.shape[1]\n",
    "            adjusted_landmarks[:,1]= adjusted_landmarks[:,1]*resized_patch.shape[0]\n",
    "            \n",
    "            \n",
    "            for points in adjusted_landmarks:\n",
    "                cv2.circle(resized_patch,(int(points[0]),int(points[1])), 3, (255,255,255), -1)\n",
    "                print (points)'''\n",
    "            \n",
    "            save_path= split_type+'_patch_images/'+patch_name\n",
    "            cv2.imwrite(save_path, patch_image)\n",
    "            \n",
    "    \n",
    "            df= df.append(adjusted_landmarks_dict, ignore_index=True) \n",
    "            \n",
    "            \n",
    "    csv_file= split_type+ '_patches_gnd'+ \".csv\"\n",
    "    df.to_csv(csv_file,index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_landmarks_position (patch_image, box_coordinates,x,y,w,h, increase_w =10, increase_h= 10):\n",
    "    \n",
    "    box_coordinates = box_coordinates.astype(float)\n",
    "\n",
    "    \n",
    "    # re-adjust the landmark coordinates in relation to single patch\n",
    "    box_coordinates[:,0]= (box_coordinates[:,0]-(x-increase_w))/ patch_image.shape[1]\n",
    "    box_coordinates[:,1]= (box_coordinates[:,1]-(y-increase_h))/ patch_image.shape[0]\n",
    "      \n",
    "    return box_coordinates\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_patch(image, x,y,w,h, increase_w = 10, increase_h=10):\n",
    "    img_copy= np.copy(image)\n",
    "    patch_image = img_copy[y-increase_h:y+h+increase_h,x-increase_w:x+w+increase_w]\n",
    "    return patch_image\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this to generate patch images for train and validation, and a correponding csv file with landmark position (groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"C:/Users/Brinda Khanal/Documents/Bidur Git Repo/Spine_Challenge/Object detection/\"\n",
    "train_data_directory = os.path.join(ROOT_PATH, \"data/train/\")\n",
    "val_data_directory = os.path.join(ROOT_PATH, \"data/val/\")\n",
    "train_label_directory=os.path.join(ROOT_PATH, \"data/labels/train/\")\n",
    "val_label_directory =os.path.join(ROOT_PATH, \"data/labels/val/\")\n",
    "\n",
    "\n",
    "\n",
    "make_csv_landmark(train_data_directory,os.path.join(train_label_directory,'filenames.csv'),\n",
    "         os.path.join(train_label_directory,'landmarks.csv'), 'train')\n",
    "\n",
    "\n",
    "make_csv_landmark(val_data_directory,os.path.join(val_label_directory,'filenames.csv'),\n",
    "         os.path.join(val_label_directory,'landmarks.csv'), 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Ground Truth for Combined Landmarks detection ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spine_image(image_directory, filenames_csv, landmarks_csv, save_path, split_type= 'train'):\n",
    "    \n",
    "    landmarks_data= pd.read_csv(landmarks_csv,header= None)\n",
    "    filename_labels= pd.read_csv(filenames_csv,header= None)\n",
    "    \n",
    "\n",
    "    \n",
    "    for i, names in enumerate(filename_labels.iloc[:,0]):\n",
    "\n",
    "        img = cv2.imread(image_directory+names)\n",
    "        print (names)\n",
    "        #print (\"image shape\",img.shape)\n",
    "        landmarks = landmarks_data.loc[i].values\n",
    "        landmark = [[int(round(img.shape[1]*landmarks[m])),int(round(img.shape[0]*landmarks[m+68]))] for m in range (0,68)]\n",
    "\n",
    "        # group 4 corner landmarks to form box\n",
    "        N=4    \n",
    "        landmark_corners = [landmark[n:n+N] for n in range(0, len(landmark), N)]\n",
    "        landmark_corners= np.array(landmark_corners)\n",
    "        boxes= []\n",
    "\n",
    "        \n",
    "        blank_image= np.zeros(img.shape,np.uint8)\n",
    "        for box in landmark_corners:\n",
    "            x,y,w,h = cv2.boundingRect(box)\n",
    "            cv2.rectangle(blank_image,(x-50,y-50),(x+w+50,y+h+50),(255,255,255),-1)\n",
    "    \n",
    "        kernel = np.ones((10,10),np.uint8)\n",
    "        dilated = cv2.dilate(blank_image,kernel,iterations = 5)\n",
    "        \n",
    "        \n",
    "        masked_image = cv2.bitwise_and(img,dilated)\n",
    "        masked = np.ma.array(data= img, mask= ~dilated.astype(bool))\n",
    "        mean= np.mean(masked)\n",
    "        print (mean)\n",
    "        img[dilated==0]=mean\n",
    "        cv2.imwrite(save_path+split_type+'/'+names,img)\n",
    "        \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"C:/Users/Brinda Khanal/Documents/Bidur Git Repo/Spine_Challenge/Object detection/\"\n",
    "train_data_directory = os.path.join(ROOT_PATH, \"data/train/\")\n",
    "val_data_directory = os.path.join(ROOT_PATH, \"data/val/\")\n",
    "train_label_directory=os.path.join(ROOT_PATH, \"data/labels/train/\")\n",
    "val_label_directory =os.path.join(ROOT_PATH, \"data/labels/val/\")\n",
    "save_path='C:/Users/Brinda Khanal/Documents/Bidur Git Repo/Spine_Challenge/all landmark estimation/groundtruth for 68 landmarks detection/' \n",
    "\n",
    "\n",
    "\n",
    "generate_spine_image(train_data_directory,os.path.join(train_label_directory,'filenames.csv'),\n",
    "         os.path.join(train_label_directory,'landmarks.csv'), save_path,'train')\n",
    "\n",
    "\n",
    "generate_spine_image(val_data_directory,os.path.join(val_label_directory,'filenames.csv'),\n",
    "         os.path.join(val_label_directory,'landmarks.csv'), save_path,'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
